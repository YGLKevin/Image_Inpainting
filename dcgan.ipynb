{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kevin_v3.3-autoencoder--2k-unique-D-runt5times.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IHstAvuuPL66",
        "Vi2YB40Hp4-s",
        "BLgb19tiqP1P"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHstAvuuPL66"
      },
      "source": [
        "#### **Import Statements:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9obeCmzyPKbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa6e65c-3dbf-4072-eb0c-a40a472e0545"
      },
      "source": [
        "from collections import namedtuple\n",
        "from enum import Enum\n",
        "from PIL import Image, ImageOps\n",
        "from skimage.viewer import ImageViewer\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import glob\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import skimage\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Viewer requires Qt\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QjFpj6TNtLo"
      },
      "source": [
        "#### **Data Loading:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkHCQ5SUISF2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5245bc-ed95-4099-8222-e51779840cda"
      },
      "source": [
        "# USER CUSTOMIZATIONS - dataloader_path of images\n",
        "\n",
        "# dir_input - stores original images\n",
        "# dir_output - stores resized original images\n",
        "# dir_augmented - stores dir_output after augmentations\n",
        "dir_input = \"/content/drive/MyDrive/APS360 Project Folder/Project/3. Implementation/Trial 3/1-original/\"\n",
        "dir_output = \"/content/drive/My Drive/APS360 Project Folder/Project/3. Implementation/Trial 3/2-standarized/\"\n",
        "dir_augmented = \"/content/drive/MyDrive/APS360 Project Folder/Project/3. Implementation/Trial 3/3-augmented/\"\n",
        "\n",
        "dataloader_path = 'dataset-v3/3B-bldg-Mirror-Rotate-2K'\n",
        "dataloader_pkl_path = \"/content/drive/MyDrive/APS360 Project Folder/Project/3. Implementation/Trial 3/3B-bldg-Mirror-Rotate-2K-unique.pkl\"\n",
        "\n",
        "\n",
        "# uncomment if using Google Colab to train\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "root = '/content/drive/My Drive/APS360 Project Folder/Project/3. Implementation/Trial 3/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB6F7AlXGLf5"
      },
      "source": [
        "class DLabel(Enum):\n",
        "    FAKE = 0\n",
        "    REAL = 1\n",
        "\n",
        "# (x1, y1) - top left; (x2, y2) - bottom right\n",
        "RegionRect = namedtuple('RegionRect', ['x1', 'y1', 'x2', 'y2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zCsx94qU0Kn"
      },
      "source": [
        "New dataset used - https://www.kaggle.com/nitishabharathi/scene-classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSfzy7O2jZI0"
      },
      "source": [
        "# NOTE: only need to be called oce to save augmented images to folder\n",
        "# image_size_standarization(128, dir_input, dir_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVzsPxB7ja-b"
      },
      "source": [
        "augmentations = {\n",
        "    \"Greyscale\": [True, False],\n",
        "    \"Mirror\": [\"ymirror\", None],\n",
        "    \"Rotation\": [180, None],\n",
        "}\n",
        "\n",
        "# augmentations = {\n",
        "#     \"Mirror\": [\"ymirror\", None],\n",
        "#     \"Rotation\": [180, None],\n",
        "# }\n",
        "\n",
        "# augmentations = {\n",
        "#     \"Greyscale\": [True, False],\n",
        "#     \"Mirror\": [\"ymirror\", None],\n",
        "# }\n",
        "\n",
        "# image_augmentation(augmentations, dir_output, dir_augmented)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTM8jWTtSiJ1"
      },
      "source": [
        "# Custom Dataset Class\n",
        "\n",
        "class InpaintingDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_names = os.listdir(image_dir)\n",
        "        self.image_names.sort()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        image_names = os.path.join(self.image_dir, self.image_names[idx])\n",
        "        image = Image.open(image_names)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "# Create InpaintingDataset instance\n",
        "dataset = None\n",
        "# dataset = InpaintingDataset(dataloader_path, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZDnRcwDeDP6"
      },
      "source": [
        "def data_loader(bs=2, path=dataloader_path, t_ind=0.75, v_ind=0.15, o_num=2, dataset=dataset):\n",
        "    \"\"\"\n",
        "    Returns a tuple with 4 elements that is ordered as following:\n",
        "    ['training', 'validation', 'testing', 'overfitting']\n",
        "\n",
        "    Each element contains loaded datasets\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    loader = []\n",
        "    t_end = int(len(dataset) * t_ind)\n",
        "    v_end = int(len(dataset) * (t_ind + v_ind))\n",
        "\n",
        "    np.random.seed(1500)  # Fixed numpy random seed for reproducible shuffling\n",
        "    np.random.shuffle(dataset)\n",
        "    data_split = [\n",
        "        dataset[:t_end]\n",
        "        , dataset[t_end:v_end]\n",
        "        , dataset[v_end:]\n",
        "        , dataset[:o_num]\n",
        "    ]\n",
        "    print(len(dataset))\n",
        "    load_order = ['training', 'validation', 'testing', 'overfitting']\n",
        "    order = 0\n",
        "    for split in data_split:\n",
        "        print(f\"Loading images for {load_order[order]}. Images to be loaded: {len(data_split[order])}\")\n",
        "        loader.append(DataLoader(split, batch_size=bs, shuffle = True))\n",
        "        order += 1\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(\"Total Time Elapsed: {:.2f} seconds\".format(elapsed_time))\n",
        "\n",
        "    return tuple(loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzDg3TZ7eLdr"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# dataset_combined = create_combined_list(mask_size=24, image_size=128)\n",
        "#\n",
        "# with open(dataloader_pkl_path, 'wb') as output:\n",
        "#     pickle.dump(dataset_combined, output, 4)\n",
        "\n",
        "with open(dataloader_pkl_path, 'rb') as input:\n",
        "    dataset_combined = pickle.load(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT0qG7_EN2i-"
      },
      "source": [
        "#### **Generator:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWvr4aEHDMhK"
      },
      "source": [
        "# # ======================== Architecture 1: Encoder + Dilation========================\n",
        "# class Generator(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Generator, self).__init__()\n",
        "#         self.encoder = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=4,  out_channels=32, kernel_size=2, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(32),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Conv2d(in_channels=32,  out_channels=64, kernel_size=2, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(64),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Conv2d(in_channels=64,  out_channels=128, kernel_size=2, stride=2, padding=1),\n",
        "#             nn.BatchNorm2d(128),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Conv2d(in_channels=128,  out_channels=256, kernel_size=2, stride=2, padding=1),\n",
        "#             nn.BatchNorm2d(256),\n",
        "#             nn.ReLU(True),\n",
        "#         )\n",
        "\n",
        "#         self.dilation = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=256,  out_channels=256, kernel_size=2, stride=1, padding=1, dilation=4),\n",
        "#             nn.BatchNorm2d(256),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Conv2d(in_channels=256,  out_channels=256, kernel_size=2, stride=1, padding=0, dilation=8),\n",
        "#             nn.BatchNorm2d(256),\n",
        "#             nn.ReLU(True),\n",
        "#         )\n",
        "\n",
        "#         # decoder here is used for decreasing number of channels, not for increasing dimension of the image\n",
        "#         self.decoder = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=256,  out_channels=128, kernel_size=1, stride=1, padding=0),\n",
        "#             nn.BatchNorm2d(128),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Conv2d(in_channels=128,  out_channels=64, kernel_size=1, stride=1, padding=0),\n",
        "#             nn.BatchNorm2d(64),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Conv2d(in_channels=64,  out_channels=32, kernel_size=1, stride=1, padding=0),\n",
        "#             nn.BatchNorm2d(32),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Conv2d(in_channels=32,  out_channels=3, kernel_size=1, stride=1, padding=0),\n",
        "#             nn.Tanh()\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.encoder(x)\n",
        "#         x = self.dilation(x)\n",
        "#         x = self.decoder(x)\n",
        "#         return x\n",
        "\n",
        "# ======================== Architecture 2: Autoencoder + Dilation ========================\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=4,  out_channels=32, kernel_size=2, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=32,  out_channels=64, kernel_size=2, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=64,  out_channels=128, kernel_size=2, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=128,  out_channels=256, kernel_size=2, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        self.dilation = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=256,  out_channels=256, kernel_size=2, stride=1, padding=1, dilation=4),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=256,  out_channels=256, kernel_size=2, stride=1, padding=1, dilation=8),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=256,  out_channels=256, kernel_size=2, stride=1, padding=1, dilation=16),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=256,  out_channels=128, kernel_size=1, stride=1, padding=0, output_padding=0),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(in_channels=128,  out_channels=64, kernel_size=1, stride=1, padding=0, output_padding=0),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(in_channels=64,  out_channels=32, kernel_size=1, stride=1, padding=0, output_padding=0),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(in_channels=32,  out_channels=3, kernel_size=2, stride=2, padding=0, output_padding=0),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.dilation(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRJ3o-BlN6Qs"
      },
      "source": [
        "#### **Discriminator:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8bxesS9hiW-J"
      },
      "source": [
        "# 2 convolutional layers, this perfomed better than the 5 layered architecture\n",
        "# during the overfitting test\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.main = nn.Sequential (\n",
        "        nn.Conv2d(in_channels=3,  out_channels=8, kernel_size=5, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(8),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5, stride=2, padding=0),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.LeakyReLU()\n",
        "    )\n",
        "    self.fc1 = nn.Sequential (\n",
        "        nn.Linear(16*30*30, 128),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Linear(128,1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.main(x)\n",
        "    x = x.view(-1, 16*30*30)\n",
        "    x = self.fc1(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi2YB40Hp4-s"
      },
      "source": [
        "#### **Accuracy functions:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQNin6rTqD7Z"
      },
      "source": [
        "def compute_G_accuracy(G_model, D_model, dataset, is_use_cuda):\n",
        "    acc, total = 0, 0\n",
        "    for i, data in enumerate(dataset, 0):\n",
        "        original_image, masked_image, _, _ = data\n",
        "\n",
        "        masked_image = masked_image[0]\n",
        "        original_image = original_image[0]\n",
        "\n",
        "        if is_use_cuda and torch.cuda.is_available():\n",
        "            masked_image = masked_image.cuda()\n",
        "            original_image = original_image.cuda()\n",
        "\n",
        "        # get output from generator\n",
        "        G_output = G_model(masked_image)\n",
        "        G_output = (G_output * 127.5) + 127.5\n",
        "        G_output = F.pad(G_output, pad=[52, 52, 52, 52], mode='constant', value=0)\n",
        "\n",
        "        G_output[:, :, 0:128, 0:52] = original_image[:, :, 0:128, 0:52]\n",
        "        G_output[:, :, 0:128, 76:128] = original_image[:, :, 0:128, 76:128]\n",
        "        G_output[:, :, 0:52, 52:76] = original_image[:, :, 0:52, 52:76]\n",
        "        G_output[:, :, 76:128, 52:76] = original_image[:, :, 76:128, 52:76]\n",
        "\n",
        "        # get classification result from  discriminator\n",
        "        D_output = D_model(G_output)\n",
        "        D_output = D_output.detach().cpu().numpy()\n",
        "\n",
        "        # calculate acc for image classification\n",
        "        for i in range(D_output.shape[0]):\n",
        "            acc += int(D_output[i] > 0.5)\n",
        "            total += 1\n",
        "\n",
        "        return acc / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo27_ToAqJAO"
      },
      "source": [
        "def compute_D_accuracy(G_model, D_model, dataset, is_use_cuda):\n",
        "    acc_real, acc_fake, total_real, total_fake, total_acc = 0, 0, 0 ,0, 0\n",
        "\n",
        "    # iterate across dataset batches to obtain number of correct classifications\n",
        "    for i, data in enumerate(dataset, 0):\n",
        "        original_image, masked_image, _, _ = data \n",
        "\n",
        "        masked_image = masked_image[0]\n",
        "        original_image = original_image[0]\n",
        "\n",
        "        if is_use_cuda and torch.cuda.is_available():\n",
        "            masked_image = masked_image.cuda()\n",
        "            original_image = original_image.cuda()\n",
        "\n",
        "        # get output from model \n",
        "        G_output = G_model(masked_image)\n",
        "        G_output = (G_output * 127.5) + 127.5\n",
        "        G_output = F.pad(G_output, pad=[52, 52, 52, 52], mode='constant', value=0)\n",
        "\n",
        "        G_output[:, :, 0:128, 0:52] = original_image[:, :, 0:128, 0:52]\n",
        "        G_output[:, :, 0:128, 76:128] = original_image[:, :, 0:128, 76:128]\n",
        "        G_output[:, :, 0:52, 52:76] = original_image[:, :, 0:52, 52:76]\n",
        "        G_output[:, :, 76:128, 52:76] = original_image[:, :, 76:128, 52:76]\n",
        "\n",
        "        # get loss for real images\n",
        "        output_real = D_model(original_image)\n",
        "        numpy_output_real = output_real.detach().cpu().numpy()\n",
        "        # calculate acc for real image classification\n",
        "        for i in range(numpy_output_real.shape[0]):\n",
        "            acc_real += int(numpy_output_real[i] > 0.5)\n",
        "            total_real += 1\n",
        "\n",
        "        # get loss for fake images\n",
        "        output_fake = D_model(G_output)\n",
        "        numpy_output_fake = output_fake.detach().cpu().numpy()\n",
        "        # calculate acc for fake image classification\n",
        "        for i in range(numpy_output_fake.shape[0]):\n",
        "            acc_fake += int(numpy_output_fake[i] < 0.5)\n",
        "            total_fake += 1\n",
        "\n",
        "        # calculate total accuracy\n",
        "        total_acc = (acc_real + acc_fake) / (total_real + total_fake)\n",
        "        return total_acc              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLgb19tiqP1P"
      },
      "source": [
        "#### **Loss functions:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoi_0M0mqTLL"
      },
      "source": [
        "def compute_G_loss(G_model, D_model, criterion, dataset, is_use_cuda, device):\n",
        "    i = 0\n",
        "    total_loss = 0.0\n",
        "    \n",
        "    for i, data in enumerate(dataset, 0):\n",
        "        original_image, masked_image, _, _ = data\n",
        "\n",
        "        masked_image = masked_image[0]\n",
        "        original_image = original_image[0]\n",
        "\n",
        "        if is_use_cuda and torch.cuda.is_available():\n",
        "            masked_image = masked_image.cuda()\n",
        "            original_image = original_image.cuda()\n",
        "\n",
        "        # get output from generator\n",
        "        G_output = G_model(masked_image)\n",
        "        G_output = (G_output * 127.5) + 127.5\n",
        "        G_output = F.pad(G_output, pad=[52, 52, 52, 52], mode='constant', value=0)\n",
        "\n",
        "        G_output[:, :, 0:128, 0:52] = original_image[:, :, 0:128, 0:52]\n",
        "        G_output[:, :, 0:128, 76:128] = original_image[:, :, 0:128, 76:128]\n",
        "        G_output[:, :, 0:52, 52:76] = original_image[:, :, 0:52, 52:76]\n",
        "        G_output[:, :, 76:128, 52:76] = original_image[:, :, 76:128, 52:76]\n",
        "\n",
        "        # get output from discriminator\n",
        "        D_output = D_model(G_output)\n",
        "\n",
        "        # get loss for each output and the total loss\n",
        "        loss = criterion(D_output, generate_binary_label(DLabel.REAL, len(D_output), device))\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    # get average loss from total loss\n",
        "    total_loss = float(total_loss) / (i+1)\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GRo50nCqYU9"
      },
      "source": [
        "def compute_D_loss(G_model, D_model, criterion, dataset, is_use_cuda, device):\n",
        "    i = 0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(dataset, 0):\n",
        "        original_image, masked_image, _, _ = data \n",
        "\n",
        "        masked_image = masked_image[0]\n",
        "        original_image = original_image[0]\n",
        "\n",
        "        if is_use_cuda and torch.cuda.is_available():\n",
        "            masked_image = masked_image.cuda()\n",
        "            original_image = original_image.cuda()\n",
        "\n",
        "        # get output images from generator\n",
        "        G_output = G_model(masked_image)  \n",
        "        G_output = (G_output * 127.5) + 127.5\n",
        "        G_output = F.pad(G_output, pad=[52, 52, 52, 52], mode='constant', value=0)\n",
        "\n",
        "        G_output[:, :, 0:128, 0:52] = original_image[:, :, 0:128, 0:52]\n",
        "        G_output[:, :, 0:128, 76:128] = original_image[:, :, 0:128, 76:128]\n",
        "        G_output[:, :, 0:52, 52:76] = original_image[:, :, 0:52, 52:76]\n",
        "        G_output[:, :, 76:128, 52:76] = original_image[:, :, 76:128, 52:76]\n",
        "\n",
        "        # get loss for real images\n",
        "        output_real = D_model(original_image)\n",
        "        loss_real = criterion(output_real, generate_binary_label(DLabel.REAL, len(output_real), device))\n",
        "\n",
        "        # get loss for fake images\n",
        "        output_fake = D_model(G_output)\n",
        "        loss_fake = criterion(output_fake, generate_binary_label(DLabel.FAKE, len(output_fake), device))\n",
        "\n",
        "        # get total loss\n",
        "        D_loss = loss_real.item() + loss_fake.item()\n",
        "        total_loss += D_loss\n",
        "        \n",
        "    # get average loss from total loss\n",
        "    total_loss = float(total_loss) / (i+1)\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2ASCjK3FshA"
      },
      "source": [
        "#### **Helper Functions:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H006VdUeFwOs"
      },
      "source": [
        "def generate_binary_label(dlabel, n, device):\n",
        "    if dlabel.value == 0:\n",
        "        return torch.zeros([n, 1], device=device)\n",
        "    elif dlabel.value == 1:\n",
        "        return torch.ones([n, 1], device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYZEug9HF98j"
      },
      "source": [
        "def plot_training_curves(train_acc_csv_path=None, train_loss_csv_path=None,\n",
        "                         valid_acc_csv_path=None, valid_loss_csv_path=None):\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # plot 1 - Training vs Validation Accuracy\n",
        "    if train_acc_csv_path or valid_acc_csv_path:\n",
        "        if train_acc_csv_path:\n",
        "            train_acc = np.loadtxt(train_acc_csv_path, delimiter=',').transpose()\n",
        "            plt.plot(train_acc[0], train_acc[1], label=\"Train\")\n",
        "\n",
        "        if valid_acc_csv_path:\n",
        "            val_acc = np.loadtxt(valid_acc_csv_path, delimiter=',').transpose()\n",
        "            plt.plot(val_acc[0], val_acc[1], label=\"Validation\")\n",
        "\n",
        "        plt.title(\"Training Curve - Accuracy\")\n",
        "        plt.xlabel(\"Iterations\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.legend(loc='best')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    # plot 2 - Training vs Validation Loss\n",
        "    if train_loss_csv_path or valid_loss_csv_path:\n",
        "        if train_loss_csv_path:\n",
        "            train_loss = np.loadtxt(train_loss_csv_path, delimiter=',').transpose()\n",
        "            plt.plot(train_loss[0], train_loss[1], label=\"Train\")\n",
        "\n",
        "        if valid_loss_csv_path:\n",
        "            val_loss = np.loadtxt(valid_loss_csv_path, delimiter=',').transpose()\n",
        "            plt.plot(val_loss[0], val_loss[1], label=\"Validation\")\n",
        "\n",
        "        plt.title(\"Training Curve - Loss\")\n",
        "        plt.xlabel(\"Iterations\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend(loc='best')\n",
        "\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wawqjp8YMm7S"
      },
      "source": [
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccEWRoLKlI_p"
      },
      "source": [
        "# pixel by pixel loss \n",
        "def pixel_loss(original_img, generated_img, use_cuda=False):\n",
        "  \"\"\" Returns the loss from pixel by pixel comaparison for one image.\n",
        "      orginal_img - the img from dataset\n",
        "      generated_img - the img produced by generator\n",
        "  \"\"\"\n",
        "  if use_cuda and torch.cuda.is_available():\n",
        "    original_img = original_img.cuda()\n",
        "    generated_img = generated_img.cuda()\n",
        "\n",
        "  difference = (original_img - generated_img)**2\n",
        "  diff = (torch.sum(difference))\n",
        "\n",
        "  return diff.item()\n",
        "\n",
        "def batch_pixel_loss(original_imgs, generated_imgs):\n",
        "  \"\"\" Returns the loss from pixel by pixel comaparison for one batch.\n",
        "      orginal_img - the batch of imgs from dataset\n",
        "      generated_img - the batch of imgs produced by generator\n",
        "  \"\"\"\n",
        "\n",
        "  loss = 0\n",
        "  for i in range(original_imgs.shape[0]):\n",
        "    loss += pixel_loss(original_imgs[i], generated_imgs[i])\n",
        "  \n",
        "  return loss/original_imgs.shape[0]\n",
        "\n",
        "\n",
        "# pixel by pixel accuracy \n",
        "def pixel_accuracy(original_img, generated_img, use_cuda=False):\n",
        "  \"\"\" Returns the accuracy from pixel by pixel comaparison for one image.\n",
        "      orginal_img - the img from dataset\n",
        "      generated_img - the img produced by generator\n",
        "  \"\"\"\n",
        "  if use_cuda and torch.cuda.is_available():\n",
        "    original_img = original_img.cuda()\n",
        "    generated_img = generated_img.cuda()\n",
        "\n",
        "  difference = original_img - generated_img\n",
        "  difference = torch.abs(difference)\n",
        "  # print(original_img)\n",
        "  # print(generated_img)\n",
        "  # print(torch.sum(difference))\n",
        "  diff = 1 - (torch.sum(difference)/(24*24))\n",
        "\n",
        "  return diff.item()\n",
        "\n",
        "def batch_pixel_accuracy(original_imgs, generated_imgs):\n",
        "  \"\"\" Returns the accuracy from pixel by pixel comaparison for one batch.\n",
        "      orginal_img - the batch of imgs from dataset\n",
        "      generated_img - the batch of imgs produced by generator\n",
        "  \"\"\"\n",
        "\n",
        "  accuracy = 0\n",
        "  for i in range(original_imgs.shape[0]):\n",
        "    accuracy += pixel_accuracy(original_imgs[i], generated_imgs[i])\n",
        "  \n",
        "  return accuracy/original_imgs.shape[0]\n",
        "\n",
        "def compute_pixel_loss_accuracy(G_model, dataset, is_use_cuda):\n",
        "    loss, acc, total = 0, 0, 0\n",
        "    for i, data in enumerate(dataset, 0):\n",
        "        original_image, masked_image, _, _ = data\n",
        "\n",
        "        masked_image = masked_image[0]\n",
        "        original_image = original_image[0][:, :, 52:76, 52:76]\n",
        "\n",
        "        if is_use_cuda and torch.cuda.is_available():\n",
        "            masked_image = masked_image.cuda()\n",
        "            original_image = original_image.cuda()\n",
        "\n",
        "        # get output from generator\n",
        "        G_output = G_model(masked_image)\n",
        "        G_output = (G_output * 127.5) + 127.5\n",
        "\n",
        "        loss += batch_pixel_loss(original_image/255, G_output/255)\n",
        "        acc += batch_pixel_accuracy(original_image/255, G_output/255)\n",
        "        total += 1\n",
        "\n",
        "    return (loss/total, acc/total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V0bH9AxN_Kz"
      },
      "source": [
        "#### **Training:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sno4VRaOBu0"
      },
      "source": [
        "def train_dcgan(g_model, d_model,\n",
        "                train_dataset, valid_dataset, nepochs, id,\n",
        "                g_criterion=None, d_criterion=None,\n",
        "                g_niter_freeze=None, d_niter_freeze=None,\n",
        "                g_nupdate_per_iter=None, d_nupdate_per_iter=None,\n",
        "                adam_g_lrate=None, adam_g_betas=None, adam_g_eps=None,\n",
        "                adam_g_weight_decay=None, adam_g_amsgrad=None,\n",
        "                adam_d_lrate=None, adam_d_betas=None, adam_d_eps=None,\n",
        "                adam_d_weight_decay=None, adam_d_amsgrad=None,\n",
        "                nbatches_per_dpt=None, nbatches_per_plot_g_out=None,\n",
        "                nepochs_per_model_save=None,\n",
        "                nimgs_per_plot_g_out=None,\n",
        "                model_dir=None, is_use_cuda=None):\n",
        "    import datetime\n",
        "    import os\n",
        "\n",
        "    device = 'cuda' if is_use_cuda else 'cpu'\n",
        "\n",
        "    # get batch size\n",
        "    batch_size = None\n",
        "    for e in train_dataset:\n",
        "      batch_size = len(e[0][0])\n",
        "      break\n",
        "\n",
        "    # set default args\n",
        "    if g_criterion is None:\n",
        "        g_criterion = nn.BCELoss()\n",
        "    if d_criterion is None:\n",
        "        d_criterion = nn.BCELoss()\n",
        "\n",
        "    if g_niter_freeze is None:\n",
        "        g_niter_freeze = 0\n",
        "    if d_niter_freeze is None:\n",
        "        d_niter_freeze = 0\n",
        "\n",
        "    if g_nupdate_per_iter is None:\n",
        "        g_nupdate_per_iter = 1\n",
        "    if d_nupdate_per_iter is None:\n",
        "        d_nupdate_per_iter = 1\n",
        "\n",
        "    if adam_g_lrate is None:\n",
        "        adam_g_lrate = 0.001\n",
        "    if adam_g_betas is None:\n",
        "        adam_g_betas = (0.9, 0.999)\n",
        "    if adam_g_eps is None:\n",
        "        adam_g_eps = 1e-08\n",
        "    if adam_g_weight_decay is None:\n",
        "        adam_g_weight_decay = 0\n",
        "    if adam_g_amsgrad is None:\n",
        "        adam_g_amsgrad = False\n",
        "\n",
        "    if adam_d_lrate is None:\n",
        "        adam_d_lrate = 0.001\n",
        "    if adam_d_betas is None:\n",
        "        adam_d_betas = (0.9, 0.999)\n",
        "    if adam_d_eps is None:\n",
        "        adam_d_eps = 1e-08\n",
        "    if adam_d_weight_decay is None:\n",
        "        adam_d_weight_decay = 0\n",
        "    if adam_d_amsgrad is None:\n",
        "        adam_d_amsgrad = False\n",
        "\n",
        "    if nbatches_per_plot_g_out is None:\n",
        "        nbatches_per_plot_g_out = 25\n",
        "    if nbatches_per_dpt is None:\n",
        "        nbatches_per_dpt = 5\n",
        "    if nepochs_per_model_save is None:\n",
        "        nepochs_per_model_save = 50\n",
        "    if nimgs_per_plot_g_out is None:\n",
        "        nimgs_per_plot_g_out = min(10, batch_size)\n",
        "    else:\n",
        "        nimgs_per_plot_g_out = min(nimgs_per_plot_g_out, batch_size)\n",
        "    if model_dir is None:\n",
        "        model_dir = \"trained_models\"\n",
        "    if is_use_cuda is None:\n",
        "        is_use_cuda = False\n",
        "\n",
        "    # keep track of time elapsed\n",
        "    start_time = time.time()\n",
        "\n",
        "    # create model directory if does not exist\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # store hyperparameter values to txt file\n",
        "    with open(rf\"{model_dir}\\{id}_hyperparameter_specs.txt\", 'w') as file:\n",
        "        file.write(f\"timestamp: {datetime.datetime.now()}\")\n",
        "        file.write(f\"\\n\")\n",
        "        file.write(f\"model id: {id}\\n\")\n",
        "        file.write(f\"batch size: {batch_size}\\n\")\n",
        "        file.write(f\"number of batches: {len(train_dataset)}\\n\")\n",
        "        file.write(f\"number of epochs: {nepochs}\\n\")\n",
        "        file.write(f\"number of batches per datapoint: {nbatches_per_dpt}\\n\")\n",
        "        file.write(f\"number of batches per plot generator output: {nbatches_per_plot_g_out}\\n\")\n",
        "        file.write(f\"number of epochs per model save: {nepochs_per_model_save}\\n\")\n",
        "        file.write(f\"number of images per plot generator output: {nimgs_per_plot_g_out}\\n\")\n",
        "        file.write(f\"\\n\")\n",
        "        file.write(f\"GENERATOR\\n\")\n",
        "        file.write(f\"\\t number of iterations to freeze: {g_niter_freeze}\\n\")\n",
        "        file.write(f\"\\t number of updates per iteration: {g_nupdate_per_iter}\\n\")\n",
        "        file.write(f\"\\t learning rate: {adam_g_lrate}\\n\")\n",
        "        file.write(f\"\\t betas: {adam_g_betas}\\n\")\n",
        "        file.write(f\"\\t eps: {adam_g_eps}\\n\")\n",
        "        file.write(f\"\\t weight decay: {adam_g_weight_decay}\\n\")\n",
        "        file.write(f\"\\t amsgrad: {adam_g_amsgrad}\\n\")\n",
        "        file.write(f\"\\n\")\n",
        "        file.write(f\"GLOBAL DISCRIMINATOR\\n\")\n",
        "        file.write(f\"\\t number of iterations to freeze: {d_niter_freeze}\\n\")\n",
        "        file.write(f\"\\t number of updates per iteration: {d_nupdate_per_iter}\\n\")\n",
        "        file.write(f\"\\t learning rate: {adam_d_lrate}\\n\")\n",
        "        file.write(f\"\\t betas: {adam_d_betas}\\n\")\n",
        "        file.write(f\"\\t eps: {adam_d_eps}\\n\")\n",
        "        file.write(f\"\\t weight decay: {adam_d_weight_decay}\\n\")\n",
        "        file.write(f\"\\t amsgrad: {adam_d_amsgrad}\\n\")\n",
        "\n",
        "    # set Adam optimizer for each model\n",
        "    g_optimizer = torch.optim.Adam(g_model.parameters(),\n",
        "                                   lr=adam_g_lrate, betas=adam_g_betas,\n",
        "                                   eps=adam_g_eps, weight_decay=adam_g_weight_decay,\n",
        "                                   amsgrad=adam_g_amsgrad)\n",
        "    d_optimizer = torch.optim.Adam(d_model.parameters(),\n",
        "                                   lr=adam_d_lrate, betas=adam_d_betas,\n",
        "                                   eps=adam_d_eps, weight_decay=adam_d_weight_decay,\n",
        "                                   amsgrad=adam_d_amsgrad)\n",
        "\n",
        "    # train model\n",
        "    train_acc_g = list()\n",
        "    train_loss_g = list()\n",
        "    val_acc_g = list()\n",
        "    val_loss_g = list()\n",
        "    train_acc_d = list()\n",
        "    train_loss_d = list()\n",
        "    val_acc_d = list()\n",
        "    val_loss_d = list()\n",
        "    train_pixel_acc_g = list()\n",
        "    train_pixel_loss_g = list()\n",
        "    val_pixel_acc_g = list()\n",
        "    val_pixel_loss_g = list()\n",
        "    niter = nepochs * len(train_dataset)\n",
        "    iter = 0\n",
        "    for epoch in range(1, nepochs + 1):\n",
        "        # iterate through each batch in training dataset\n",
        "        for ori_imgs, pre_proc_imgs, _, _ in train_dataset:\n",
        "            iter += 1\n",
        "\n",
        "            ori_imgs = ori_imgs[0]\n",
        "            pre_proc_imgs = pre_proc_imgs[0]\n",
        "\n",
        "            # store in GPU for CUDA processing\n",
        "            if is_use_cuda and torch.cuda.is_available():\n",
        "                ori_imgs = ori_imgs.cuda()\n",
        "                pre_proc_imgs = pre_proc_imgs.cuda()\n",
        "\n",
        "            # A) train global discriminator\n",
        "            #d_loss = None\n",
        "            if (iter - 1) % (d_niter_freeze + 1) == 0:\n",
        "                for _ in range(d_nupdate_per_iter):\n",
        "                    g_output = g_model(pre_proc_imgs)\n",
        "                    g_output = (g_output * 127.5) + 127.5\n",
        "                    g_output = F.pad(g_output, pad=[52, 52, 52, 52], mode='constant', value=0)\n",
        "\n",
        "                    g_output[:, :, 0:128, 0:52] = ori_imgs[:, :, 0:128, 0:52]\n",
        "                    g_output[:, :, 0:128, 76:128] = ori_imgs[:, :, 0:128, 76:128]\n",
        "                    g_output[:, :, 0:52, 52:76] = ori_imgs[:, :, 0:52, 52:76]\n",
        "                    g_output[:, :, 76:128, 52:76] = ori_imgs[:, :, 76:128, 52:76]\n",
        "                    #cropped = torch.round(cropped)\n",
        "\n",
        "                    d_model.zero_grad()\n",
        "                    # A.a) real - data from training set\n",
        "                    d_output_real = d_model(ori_imgs)\n",
        "                    d_loss_real = d_criterion(d_output_real,\n",
        "                                              generate_binary_label(DLabel.REAL,\n",
        "                                                                    len(d_output_real),\n",
        "                                                                    device))\n",
        "                    d_loss_real.backward()\n",
        "\n",
        "                    # A.b) fake - data outputted by generator model\n",
        "                    d1_output_fake = d_model(g_output)\n",
        "                    d1_loss_fake = d_criterion(d1_output_fake,\n",
        "                                               generate_binary_label(DLabel.FAKE,\n",
        "                                                                     len(d1_output_fake),\n",
        "                                                                     device))\n",
        "\n",
        "                    d1_loss_fake.backward()\n",
        "\n",
        "                    d_loss = d_loss_real + d1_loss_fake\n",
        "                    d_optimizer.step()\n",
        "\n",
        "            # B) train generator\n",
        "            # g_loss = None\n",
        "            # g_output = None\n",
        "            if (iter - 1) % (g_niter_freeze + 1) == 0:\n",
        "                for _ in range(g_nupdate_per_iter):\n",
        "                    g_model.zero_grad()\n",
        "                    g_output = g_model(pre_proc_imgs)\n",
        "                    g_output = (g_output * 127.5) + 127.5\n",
        "                    g_output = F.pad(g_output, pad=[52, 52, 52, 52], mode='constant', value=0)\n",
        "\n",
        "                    g_output[:, :, 0:128, 0:52] = ori_imgs[:, :, 0:128, 0:52]\n",
        "                    g_output[:, :, 0:128, 76:128] = ori_imgs[:, :, 0:128, 76:128]\n",
        "                    g_output[:, :, 0:52, 52:76] = ori_imgs[:, :, 0:52, 52:76]\n",
        "                    g_output[:, :, 76:128, 52:76] = ori_imgs[:, :, 76:128, 52:76]\n",
        "\n",
        "                    d_output_fake_2 = d_model(g_output)\n",
        "\n",
        "                    g_loss = g_criterion(d_output_fake_2,\n",
        "                                         generate_binary_label(DLabel.REAL,\n",
        "                                                               len(d_output_fake_2),\n",
        "                                                               device))\n",
        "                    g_loss.backward()\n",
        "                    g_optimizer.step()\n",
        "\n",
        "            # store datapoint w/ frequency based on 'nbatches_per_dpt' and at last iteration\n",
        "            if (iter - 1) % nbatches_per_dpt == 0 or iter == niter:\n",
        "                # compute and record accuracy / loss\n",
        "                # a) generator\n",
        "                train_acc_g.append((iter, compute_G_accuracy(g_model, d_model,\n",
        "                                                             train_dataset,\n",
        "                                                             is_use_cuda=is_use_cuda)))\n",
        "                train_loss_g.append((iter, g_loss.item()))\n",
        "                val_acc_g.append((iter, compute_G_accuracy(g_model, d_model,\n",
        "                                                           valid_dataset,\n",
        "                                                           is_use_cuda=is_use_cuda)))\n",
        "                val_loss_g.append((iter, compute_G_loss(g_model, d_model,\n",
        "                                                        g_criterion,\n",
        "                                                        valid_dataset,\n",
        "                                                        is_use_cuda=is_use_cuda, device=device)))             \n",
        "\n",
        "                # b) discriminator\n",
        "                train_acc_d.append((iter, compute_D_accuracy(g_model, d_model,\n",
        "                                                             train_dataset,\n",
        "                                                             is_use_cuda=is_use_cuda)))\n",
        "                train_loss_d.append((iter, d_loss.item()))\n",
        "                val_acc_d.append((iter, compute_D_accuracy(g_model, d_model,\n",
        "                                                           valid_dataset,\n",
        "                                                           is_use_cuda=is_use_cuda)))\n",
        "                val_loss_d.append((iter, compute_D_loss(g_model, d_model,\n",
        "                                                        d_criterion,\n",
        "                                                        valid_dataset,\n",
        "                                                        is_use_cuda=is_use_cuda, device=device)))\n",
        "                \n",
        "                # c) pixel loss\n",
        "                px_train_loss, px_train_acc = compute_pixel_loss_accuracy(g_model,\n",
        "                                                                          train_dataset,\n",
        "                                                                          is_use_cuda=is_use_cuda)\n",
        "                px_val_loss, px_val_acc = compute_pixel_loss_accuracy(g_model,\n",
        "                                                                      valid_dataset,\n",
        "                                                                      is_use_cuda=is_use_cuda)\n",
        "                train_pixel_acc_g.append((iter, px_train_acc))\n",
        "                train_pixel_loss_g.append((iter, px_train_loss))\n",
        "                val_pixel_acc_g.append((iter, px_val_acc))\n",
        "                val_pixel_loss_g.append((iter, px_val_loss))\n",
        "\n",
        "                # output accuracy / loss\n",
        "                print(f\"Epoch {epoch} | iter {iter}: \")\n",
        "                # a) generator\n",
        "                print(\"a) generator\")\n",
        "                print(f\"\\t training accuracy: {train_acc_g[-1][1]}\")\n",
        "                print(f\"\\t training loss: {train_loss_g[-1][1]}\")\n",
        "                print(f\"\\t validation accuracy: {val_acc_g[-1][1]}\")\n",
        "                print(f\"\\t validation loss: {val_loss_g[-1][1]}\")\n",
        "                # b) discriminator\n",
        "                print(\"b) discriminator\")\n",
        "                print(f\"\\t training accuracy: {train_acc_d[-1][1]}\")\n",
        "                print(f\"\\t training loss: {train_loss_d[-1][1]}\")\n",
        "                print(f\"\\t validation accuracy: {val_acc_d[-1][1]}\")\n",
        "                print(f\"\\t validation loss: {val_loss_d[-1][1]}\")\n",
        "                # c) pixel loss\n",
        "                print(\"c) pixel loss\")\n",
        "                print(f\"\\t training accuracy: {train_pixel_acc_g[-1][1]}\")\n",
        "                print(f\"\\t training loss: {train_pixel_loss_g[-1][1]}\")\n",
        "                print(f\"\\t validation accuracy: {val_pixel_acc_g[-1][1]}\")\n",
        "                print(f\"\\t validation loss: {val_pixel_loss_g[-1][1]}\")\n",
        "\n",
        "            # plot output w/ frequency based on 'nbatches_per_plot_g_out' and at last iteration\n",
        "            if (iter - 1) % nbatches_per_plot_g_out == 0 or iter == niter:\n",
        "                print(f\"Epoch {epoch} | iter {iter}: \"\n",
        "                      f\"generated img - its missing region - original img - its missing region\")\n",
        "                fig = plt.figure(figsize=(15, 15))\n",
        "                cols = 4\n",
        "                rows = nimgs_per_plot_g_out\n",
        "\n",
        "                for r in range(1, rows + 1):\n",
        "                    i = (r - 1) * cols\n",
        "\n",
        "                    # a) plot generated image\n",
        "                    fig.add_subplot(rows, cols, i + 1)\n",
        "                    plt.imshow(np.transpose(g_output[r-1].detach().cpu() / 255,\n",
        "                                            (1, 2, 0)))\n",
        "\n",
        "                    # b) plot missing region of generated image\n",
        "                    fig.add_subplot(rows, cols, i + 2)\n",
        "                    plt.imshow(np.transpose(g_output[r-1][:, 52:76, 52:76].detach().cpu() / 255,\n",
        "                                            (1, 2, 0)))\n",
        "\n",
        "                    # c) plot original image\n",
        "                    fig.add_subplot(rows, cols, i + 3)\n",
        "                    plt.imshow(np.transpose(ori_imgs[r-1].detach().cpu() / 255,\n",
        "                                            (1, 2, 0)))\n",
        "\n",
        "                    # d) plot missing region of original image\n",
        "                    fig.add_subplot(rows, cols, i + 4)\n",
        "                    plt.imshow(np.transpose(ori_imgs[r-1][:, 52:76, 52:76].detach().cpu() / 255,\n",
        "                                            (1, 2, 0)))\n",
        "                plt.show()\n",
        "\n",
        "        # save frequency of the current model (checkpoint) to a file based on\n",
        "        # 'nepochs_per_model_save' and at last epoch\n",
        "        if (epoch - 1) % nepochs_per_model_save == 0 or epoch == nepochs:\n",
        "            # a) generator\n",
        "            path = rf\"{model_dir}\\{id}_G_ne{nepochs}_niter{niter}_e{epoch}_iter{iter}\"\n",
        "            torch.save(g_model.state_dict(), path)\n",
        "            # b) global discriminator\n",
        "            path = rf\"{model_dir}\\{id}_D1_ne{nepochs}_niter{niter}_e{epoch}_iter{iter}\"\n",
        "            torch.save(d_model.state_dict(), path)\n",
        "\n",
        "    # output elapsed time\n",
        "    print('finished training')\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(\"total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
        "\n",
        "    # write the train/test loss/err into CSV file for plotting later\n",
        "    # a) generator\n",
        "    g_csv_paths = (rf\"{model_dir}\\{id}_G_train_accuracy.csv\",\n",
        "                   rf\"{model_dir}\\{id}_G_train_loss.csv\",\n",
        "                   rf\"{model_dir}\\{id}_G_val_accuracy.csv\",\n",
        "                   rf\"{model_dir}\\{id}_G_val_loss.csv\")\n",
        "    np.savetxt(g_csv_paths[0], train_acc_g, delimiter=',')\n",
        "    np.savetxt(g_csv_paths[1], train_loss_g, delimiter=',')\n",
        "    np.savetxt(g_csv_paths[2], val_acc_g, delimiter=',')\n",
        "    np.savetxt(g_csv_paths[3], val_loss_g, delimiter=',')\n",
        "    # b) discriminator\n",
        "    d1_csv_paths = (rf\"{model_dir}\\{id}_D_train_accuracy.csv\",\n",
        "                    rf\"{model_dir}\\{id}_D_train_loss.csv\",\n",
        "                    rf\"{model_dir}\\{id}_D_val_accuracy.csv\",\n",
        "                    rf\"{model_dir}\\{id}_D_val_loss.csv\")\n",
        "    np.savetxt(d1_csv_paths[0], train_acc_d, delimiter=',')\n",
        "    np.savetxt(d1_csv_paths[1], train_loss_d, delimiter=',')\n",
        "    np.savetxt(d1_csv_paths[2], val_acc_d, delimiter=',')\n",
        "    np.savetxt(d1_csv_paths[3], val_loss_d, delimiter=',')\n",
        "    # c) pixel loss\n",
        "    px_csv_paths = (rf\"{model_dir}\\{id}_PX_train_accuracy.csv\",\n",
        "                    rf\"{model_dir}\\{id}_PX_train_loss.csv\",\n",
        "                    rf\"{model_dir}\\{id}_PX_val_accuracy.csv\",\n",
        "                    rf\"{model_dir}\\{id}_PX_val_loss.csv\")\n",
        "    np.savetxt(px_csv_paths[0], train_pixel_acc_g, delimiter=',')\n",
        "    np.savetxt(px_csv_paths[1], train_pixel_loss_g, delimiter=',')\n",
        "    np.savetxt(px_csv_paths[2], val_pixel_acc_g, delimiter=',')\n",
        "    np.savetxt(px_csv_paths[3], val_pixel_loss_g, delimiter=',')\n",
        "\n",
        "    return g_csv_paths, d1_csv_paths, px_csv_paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3k7brLeOjQz"
      },
      "source": [
        "#### **Tunning:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC1LqThwOl0Z"
      },
      "source": [
        "# A. create models\n",
        "# g_model = Generator()\n",
        "g_model = Generator()\n",
        "# g_model = Generator_AD()\n",
        "# d_model = Discriminator()\n",
        "d_model = Discriminator()\n",
        "# d_model = Discriminator_3L()\n",
        "# d_model = Discriminator_3LC()\n",
        "# d_model = Discriminator_4LC_avgP()\n",
        "# d_model = Discriminator_3LC_avgP()\n",
        "\n",
        "# weight initialization\n",
        "g_model.apply(weights_init)\n",
        "d_model.apply(weights_init)\n",
        "\n",
        "is_use_cuda = True\n",
        "if is_use_cuda and torch.cuda.is_available():\n",
        "    g_model.cuda()\n",
        "    d_model.cuda()\n",
        "\n",
        "# B. create dataset\n",
        "bs = 64\n",
        "t_ind = 0.75\n",
        "v_ind = 0.15\n",
        "o_num = 1\n",
        "\n",
        "t_loader, v_loader, test_loader, overfit_loader = data_loader(bs=bs,\n",
        "                                                              t_ind=t_ind,\n",
        "                                                              v_ind=v_ind,\n",
        "                                                              o_num=o_num,\n",
        "                                                              dataset=dataset_combined)\n",
        "\n",
        "# C. train models\n",
        "train_dataset = t_loader\n",
        "valid_dataset = v_loader\n",
        "overfit_dataset = overfit_loader\n",
        "\n",
        "nepochs = 1500\n",
        "# id = \"test\"\n",
        "id = \"bldg_rgb-gED_d2L-ne1000\"\n",
        "\n",
        "g_niter_freeze = 0\n",
        "d_niter_freeze = 0\n",
        "g_nupdate_per_iter = 1\n",
        "d_nupdate_per_iter = 2\n",
        "\n",
        "adam_g_lrate = 0.00078\n",
        "adam_g_betas = None\n",
        "adam_g_eps = None\n",
        "adam_g_weight_decay = None\n",
        "adam_g_amsgrad = None\n",
        "\n",
        "adam_d_lrate = 0.00078\n",
        "adam_d_betas = None\n",
        "adam_d_eps = None\n",
        "adam_d_weight_decay = None\n",
        "adam_d_amsgrad = None\n",
        "\n",
        "nbatches_per_dpt = 1000\n",
        "nbatches_per_plot_g_out = 1000\n",
        "nepochs_per_model_save = 1000\n",
        "nimgs_per_plot_g_out = 1\n",
        "\n",
        "csv_paths = train_dcgan(g_model, d_model,\n",
        "                        train_dataset, valid_dataset, nepochs, id,\n",
        "                        g_niter_freeze=g_niter_freeze, d_niter_freeze=d_niter_freeze,\n",
        "                        g_nupdate_per_iter=g_nupdate_per_iter,\n",
        "                        d_nupdate_per_iter=d_nupdate_per_iter,\n",
        "                        adam_g_lrate=adam_g_lrate, adam_g_betas=adam_g_betas, adam_g_eps=adam_g_eps,\n",
        "                        adam_g_weight_decay=adam_g_weight_decay, adam_g_amsgrad=adam_g_amsgrad,\n",
        "                        adam_d_lrate=adam_d_lrate, adam_d_betas=adam_d_betas, adam_d_eps=adam_d_eps,\n",
        "                        adam_d_weight_decay=adam_d_weight_decay, adam_d_amsgrad=adam_d_amsgrad,\n",
        "                        nbatches_per_dpt=nbatches_per_dpt,\n",
        "                        nbatches_per_plot_g_out=nbatches_per_plot_g_out,\n",
        "                        nepochs_per_model_save=nepochs_per_model_save,\n",
        "                        nimgs_per_plot_g_out=nimgs_per_plot_g_out,\n",
        "                        is_use_cuda=is_use_cuda)\n",
        "g_csv_paths, d_csv_paths, px_csv_path = csv_paths\n",
        "\n",
        "# D. plot\n",
        "print(\"training curves - generator\")\n",
        "plot_training_curves(*g_csv_paths)\n",
        "print('')\n",
        "print(\"training curves - discriminator\")\n",
        "plot_training_curves(*d_csv_paths)\n",
        "print('')\n",
        "print(\"training curves - pixel difference\")\n",
        "plot_training_curves(*px_csv_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}